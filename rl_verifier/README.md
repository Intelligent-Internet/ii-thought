# RL Verifier
We introduce a verification framework to evaluate LLM-generated responses against ground truth, supporting our released RL dataset.
## Overview

The verification system combines established tools:

### 1. Mathematical Tasks
- **Tool**: [Math-Verify](https://github.com/huggingface/Math-Verify) (Hugging Face)
- **Description**: Evaluates mathematical expressions generated by LLMs.

### 2. Code Generation
- **Tool**: [Sandbox Fusion](https://github.com/bytedance/SandboxFusion) (ByteDance)
- **Description**: Executes LLM-generated code in a secure sandbox and verifies its correctness against test cases.

### 3. Software Engineering Tasks
- **Tool**: SWE Verifier (inspired by [facebookresearch/swe-rl](https://github.com/facebookresearch/swe-rl))
- **Description**: Compares code patches (original vs. edited code) using `difflib.SequenceMatcher`.

### 4. Other Domains (Textbooks, Medical, etc.)
- **Tool**: LLM-as-Judge
- **Description**: Uses an LLM to verify response alignment with reference answer.

## Project Structure

```
rl_verifier/
├── src/
│   ├── app/                  # FastAPI application
│   │   ├── main.py           # Main server implementation
│   │   ├── config.py         # Configuration settings
│   │   └── verifier/         # Verifier implementations
│   │       ├── math_verifier.py
│   │       ├── code_verifier.py
│   │       ├── swe_verifier.py
│   │       └── llm_judge.py
│   └── rl_verifier/          # Client package
│       ├── client.py
│       ├── utils.py
│       └── exception.py
├── scripts/                  # Utility scripts
├── example/                  # Example usage
└── tests/                    # Unit and integration tests
```

## Installation

### Prerequisites

- Python 3.10+
- Git

### Setup

```bash
# Clone the repository
git clone https://github.com/Intelligent-Internet/ii_thought
cd rl-verifier

# Create and activate a conda environment
conda create -n rl_verifier python=3.10 -y
conda activate rl_verifier

# Install dependencies
pip install -r requirements.txt

# Install the rl_verifier client package (for development)
pip install -e .
```

## Usage

### Prerequisites for Code Verification

If you plan to use the code verification feature (`code_verifiable`), you must:

1. Start the sandbox server first:
```bash
# Start the sandbox server
docker run -d  \
    --privileged \
    -p 8080:8080 \
    -e WORKER_SIZE=8 \
    tuenguyen/code_sandbox:server \
    make run-online
```

2. Configure the sandbox URL in your `.env` file:
```bash
# .env
FUSION_SANDBOX_URL=http://localhost:8080
```

Similarly, For tasks using LLM-as-Judge, configure the model and API settings in your .env file. The LLM-as-Judge is openai-compatible and can be served using frameworks like vLLM or SGLang.

```bash
# .env
LLM_JUDGE_MODEL=Qwen/Qwen2.5-32B-Instruct       # Model name
LLM_JUDGE_BASE_URL=http://localhost:8002/v1     # API endpoint (if you are using local LLM)
LLM_JUDGE_API_KEY=EMPTY                         # API key
LLM_JUDGE_MAX_TOKENS=1000                       # Max output tokens
LLM_JUDGE_TEMPERATURE=0.0                       # Sampling temperature
USE_FORMAT_VERIFIER=false                       # Whether to use format verifier
```

### Running the Server

```bash
uvicorn src.app.main:app --host 0.0.0.0 --port 8000 --workers 5
```

The server will start on `http://0.0.0.0:8000` by default.

### API Documentation

Once the server is running, you can access the interactive API documentation at:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

### API Endpoints

#### POST /reward

Computes a reward score for an LLM output based on verification information.

**Request Body:**

```json
{
  "llm_output": "The LLM-generated output to verify",
  "verification_info": "{\"type\": \"math_verifiable\", \"answer\": {\"value\": \"42\"}}"
}
```

The `verification_info` must be a JSON string containing:
- `type`: One of "math_verifiable", "code_verifiable", "swe_verifiable", or "llm_judge"
- `answer`: The reference answer to compare against (format varies by verification type)

**Response:**

```json
{
  "score": 1.0
}
```

The score is a float between 0 and 1, where 1.0 indicates the output is correct and 0.0 indicates it is incorrect.

### Client Examples

#### Using Python Requests

```python
import requests
import json

# Server URL
BASE_URL = "http://localhost:8000"

verification_info = {"type": "math_verifiable", "answer": {"value": "152"}}
llm_output = "The final answer is \( \\boxed{152} \)."

# Using Python requests
payload = {"llm_output": llm_output, "verification_info": json.dumps(verification_info)}
response = requests.post(f"{BASE_URL}/reward", json=payload).json()
print(f"Score received: {response['score']}")
```

#### Using the RL Verifier Client

```python
from rl_verifier import RLVerifierClient

# Initialize client
client = RLVerifierClient("http://localhost:8000")

# Verify output
verification_info = {"type": "math_verifiable", "answer": {"value": "152"}}
llm_output = "The final answer is \( \\boxed{152} \)."

score = client.verify(llm_output, verification_info)
print(f"Score received: {score}")
```

## Verification Types

### Math Verifier

We use [math-verify](https://github.com/huggingface/Math-Verify) to verify the correctness of mathematical solutions. The verifier extracts the final answer from the LLM output and compares it with the reference answer.

**Example:**
```python
verification_info = {
    "type": "math_verifiable", 
    "answer": {"value": "152"}
}
```

### Code Verifier

For code problems with test cases (e.g., competitive programming), we use [sandbox-fusion](https://github.com/bytedance/SandboxFusion) to execute the LLM-generated code in a secure sandbox and verify its correctness against test cases.

**Example:**
```python
verification_info = {
    "type": "code_verifiable",
    "answer": {
        "test_cases": [
            {"input": "3\n5", "output": "8"},
            {"input": "-10\n4", "output": "-6"}
        ]
    }
}
```

### SWE Verifier

For software engineering problems, we adapt the approach from [swe-rl](https://github.com/facebookresearch/swe-rl) to compare the LLM-generated code against reference implementations, focusing on functional equivalence rather than exact matching.

**Example:**
```python
verification_info = {
    "type": "swe_verifiable",
    "answer": {
        "input": "def add(a, b):\n    return a + b",
        "ground_truth": "def add_plus_one(a, b):\n    return a + b + 1"
    }
}
```

### LLM Judge

We use another LLM to judge the correctness of the output based on the reference answer. The prompt is adapted from the [HuatuoGPT-o1](https://arxiv.org/pdf/2412.18925) paper, providing a flexible way to evaluate outputs in domains where automated verification is challenging.

**Example:**
```python
verification_info = {
    "type": "llm_judge",
    "answer": {
        "value": "The patient should be treated with antibiotics."
    }
}
```

## Related Projects

- [Math-Verify](https://github.com/huggingface/Math-Verify) - A robust mathematical expression evaluation system designed for assessing Large Language Model outputs in mathematical tasks.
- [SandboxFusion](https://github.com/bytedance/SandboxFusion) - A secure sandbox for running and judging code generated by LLMs.
- [SWE-RL](https://github.com/facebookresearch/swe-rl) - Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution.
- [HuatuoGPT-o1](https://arxiv.org/pdf/2412.18925) - HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs.